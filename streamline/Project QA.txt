ğŸ”¹ 1. Can you explain your fraud detection project?

Answer:

I built a credit card fraud detection demo using machine learning and Streamlit. Since the dataset uses anonymized PCA features, I designed the app to simulate real-time prediction by allowing users to select an existing transaction. The app loads a trained model and predicts the probability of fraud, then categorizes the transaction into low, medium, or high risk based on business-defined thresholds.

ğŸ”¹ 2. Why did you use a transaction index instead of a form?

Answer:

The dataset contains PCA-transformed features (V1â€“V28), which are not human-interpretable. So instead of asking users to enter meaningless values, I used a transaction index to simulate real transactions by selecting a row from the dataset and running it through the model.

ğŸ”¹ 3. What is a â€œtransaction snapshotâ€?

Answer:

A transaction snapshot shows the full feature values of the selected transaction. It improves transparency by letting users see exactly what data the model is using for prediction.

ğŸ”¹ 4. Why did you drop the Class column before prediction?

Answer:

Class is the target label that indicates whether a transaction is fraud or not. During prediction, the model should not see the actual answer, so I remove it to avoid data leakage and ensure a fair prediction.

ğŸ”¹ 5. Why did you reindex the input using feature_names?

Answer:

During training, the model learns features in a specific order. To avoid feature mismatch during deployment, I save the feature list and reindex the input data so that the columns always match the training schema. This prevents incorrect predictions and runtime errors.

ğŸ”¹ 6. Why did you not use a pipeline?

Answer:

Instead of using a pipeline, I saved the model, scaler, and feature names separately and applied preprocessing manually in the app. This modular approach helped me clearly understand deployment issues like feature mismatch and scaling consistency, and gave me better control while debugging.

ğŸ”¹ 7. What errors did you face during deployment?

Answer:

I faced issues like â€œcould not convert string to floatâ€ and feature mismatch errors. These happened because the preprocessing in deployment did not initially match the training steps. I fixed this by aligning the input features using saved feature names and ensuring consistent preprocessing.

ğŸ”¹ 8. Why are you using probability instead of direct class prediction?

Answer:

Fraud detection is a probabilistic problem. Instead of directly predicting fraud or not, I use the probability score to classify transactions into risk levels. This allows flexible threshold tuning based on business priorities like minimizing fraud loss or reducing customer inconvenience.

ğŸ”¹ 9. Why can a fraud transaction show as low risk?

Answer:

The model assigns a likelihood score, not a final decision. Some fraud transactions have patterns similar to normal behavior, so the model gives them a lower probability. Risk labels depend on the chosen threshold, and this trade-off between recall and precision is normal in real-world fraud systems.

ğŸ”¹ 10. How did you decide the thresholds (0.35, 0.6)?

Answer:

These thresholds represent business-driven risk bands. Lower thresholds help catch more fraud but may increase false alarms, while higher thresholds reduce customer friction. In real production systems, these values are tuned using precisionâ€“recall trade-offs and cost analysis.
ğŸ”¹ What does â€œbusiness-drivenâ€ mean?

Different companies have different priorities:

Case 1 â€” Bank wants to stop fraud at any cost

They will:

Keep threshold low (like 0.3)

Catch more fraud

But may block some good customers

Case 2 â€” Bank wants smooth customer experience

They will:

Keep threshold high (like 0.7)

Fewer customers blocked

But some fraud may pass

So thresholds depend on:

What is more important â€” loss reduction or customer happiness?

ğŸ”¹ What is â€œprecisionâ€“recall trade-offâ€?

Recall = How many frauds you catch

Precision = How many alerts are actually fraud

When you:

Lower threshold â†’ recall â†‘, precision â†“

Raise threshold â†’ precision â†‘, recall â†“

The thresholds 0.35 and 0.6 were chosen to convert probability scores into actionable risk levels. They represent business-driven risk bands. Lower thresholds help catch more fraud but may increase false alarms, while higher thresholds reduce customer friction but may allow some fraud to pass. In real production systems, thresholds are tuned using precisionâ€“recall trade-offs and cost analysis to balance financial loss and customer experience.

ğŸ”¹ 11. Why did you use caching in Streamlit?

Answer:

I used @st.cache_resource and @st.cache_data to avoid reloading the model and dataset every time the app reruns. This improves performance and reduces unnecessary computation.
In Streamlit, the app reruns automatically whenever:

You move a slider

Click a button

Change any input

If you donâ€™t use caching, then every time:

The model is loaded again

The dataset is read again

Everything runs from the start

This makes the app:

Slow

Wasteful

Less professional

ğŸ”¹ What caching does

When you use cache 

Streamlit:

Saves the result in memory

Reuses it next time

Doesnâ€™t reload unless something changes

ğŸ”¹ Why two different caches?
1ï¸âƒ£ @st.cache_resource

Used for heavy objects that donâ€™t change:
ML model
Scaler
Feature list
These are:
Large
Slow to load
Same every time

So we cache them.
2ï¸âƒ£ @st.cache_data
Used for data:
CSV file
DataFrames
Queries

These may change sometimes,
but not on every rerun.

ğŸ”¹ What happens without caching?

Every click:
Reload model
Reload data
Recompute everything

This:

Increases load time
Wastes memory

ğŸ”¹ 12. What would you improve if this were a production system?

Answer:

In production, I would use a full preprocessing pipeline, add model monitoring, implement threshold tuning based on business KPIs, integrate real-time transaction streams, and add explainability tools like SHAP for model transparency.
1ï¸âƒ£ Use a full preprocessing pipeline
What you do now

You manually apply steps like:

feature alignment

scaling

encoding

In production

You would put everything inside a pipeline

So training and prediction always use exactly the same steps

This reduces bugs and deployment errors.

ğŸ‘‰ Means: more reliability
2ï¸âƒ£ Add model monitoring
What it means

After deployment, you donâ€™t just forget the model.

You continuously check:

Is accuracy dropping?

Is fraud pattern changing?

Is data distribution changing?

This is called:

Model monitoring / model drift detection

ğŸ‘‰ Means: model stays useful over time

4ï¸âƒ£ Integrate real-time transaction streams
What you do now

You select a transaction from dataset.

In production

The system would:

Receive live transactions from:

payment gateway

banking system

Predict fraud in real time

This uses:

APIs

Streaming tools (Kafka, etc.)

ğŸ‘‰ Means: real-world usage

5ï¸âƒ£ Add explainability tools like SHAP
What it means

Your model predicts:

Fraud probability = 0.72


But business asks:

Why?

SHAP helps you answer:

Which features caused this decision?

Was it amount, time, behavior?

ğŸ‘‰ Means: trust + transparency

ğŸ”¹ 13. What did you learn from this project?

Answer:

I learned how important deployment consistency is â€” especially preprocessing alignment between training and inference. I also learned that fraud detection is not just about accuracy but about balancing business impact using probability thresholds.
1ï¸âƒ£ â€œDeployment consistency is very importantâ€
What this means

When you train a model, you do things like:

Encoding

Scaling

Feature selection

If you donâ€™t do the exact same steps while using the model in the app, then:

Predictions become wrong

Errors come

Model looks bad

You learned that:

Training and deployment must follow the same preprocessing logic.

Thatâ€™s called:

Preprocessing alignment between training and inference

2ï¸âƒ£ â€œFraud detection is not just about accuracyâ€

At first you may think:

Higher accuracy = better model

But in fraud detection:

Fraud cases are very rare

A model can get 99% accuracy by predicting everything as normal

But that is useless

So you learned:

What really matters is:

Catching fraud (recall)

Reducing business loss

Not disturbing good customers

Thatâ€™s why you use:

Probability thresholds

Risk levels (Low / Medium / High)

This is called:

Balancing business impact, not just accuracy
ğŸ”¹ 14. Why is this project valuable on your resume?

Answer:

This project shows end-to-end skills â€” from model training to deployment using Streamlit, handling real-world issues like feature mismatch, class imbalance, threshold tuning, and translating technical outputs into business decisions.

